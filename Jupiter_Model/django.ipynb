{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-26T10:53:36.976786Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad86196bcc18bf9",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Daniele\\PycharmProjects\\NLP-CodeGen\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "import logging\n",
    "from transformers import BertTokenizer, GPT2Tokenizer, GPT2TokenizerFast, EncoderDecoderModel, Trainer, TrainingArguments, BertTokenizerFast\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments,EarlyStoppingCallback\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import types\n",
    "import argparse\n",
    "import logging\n",
    "from functools import partial\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertGenerationConfig,\n",
    "    BertGenerationEncoder,\n",
    "    BertTokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    EncoderDecoderConfig,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import sacrebleu\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f36395b77c9b33",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "912c46abcaf4fb72",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"DJANGO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e708dcfe48f7d1",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset= raw_dataset[\"train\"]\n",
    "val_dataset = raw_dataset[\"validation\"]\n",
    "test_dataset = raw_dataset[\"test\"]\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bdc4651-9fb2-4dea-a65c-57b94ce3713e",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['nl', 'code'],\n",
       "        num_rows: 11428\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['nl', 'code'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['nl', 'code'],\n",
       "        num_rows: 1805\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfcddc14ce2fc25",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\", use_fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d21850-a8ba-4b14-803e-d42c9661a930",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-nl', vocab_size=67028, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t67027: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9857c5071c01c5",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-nl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237e5882-0116-42b5-abf7-2aadd8bd2a8a",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(67028, 512, padding_idx=67027)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(67028, 512, padding_idx=67027)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(67028, 512, padding_idx=67027)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=67028, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a892a499a03d9f",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniele\\PycharmProjects\\NLP-CodeGen\\utils\\evaluator.py:32: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.sacre_bleu: Metric = load_metric('sacrebleu')\n"
     ]
    }
   ],
   "source": [
    "from utils import evaluator\n",
    "evaluator = evaluator.CodeGenerationEvaluator(tokenizer, device,smooth_bleu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a2615b3253b541",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "encoder_length = 32\n",
    "decoder_length = 32\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "# map data correctly\n",
    "def map_to_encoder_decoder_inputs(batch):    \n",
    "    inputs = tokenizer(batch[\"nl\"], padding=\"max_length\", truncation=True, max_length=encoder_length)\n",
    "    outputs = tokenizer(batch[\"code\"], padding=\"max_length\", truncation=True, max_length=decoder_length)\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "    \n",
    "    \"\"\"\n",
    "    # complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if mask == 0 else token for mask, token in mask_and_tokens] for mask_and_tokens in [zip(masks, labels) for masks, labels in zip(batch[\"decoder_attention_mask\"], batch[\"labels\"])]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    assert all([len(x) == encoder_length for x in inputs.input_ids])\n",
    "    assert all([len(x) == decoder_length for x in outputs.input_ids])\n",
    "    \n",
    "    return batch\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1189e006a938f58",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# make train dataset ready\n",
    "train_data = train_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=1, remove_columns=['nl', 'code'],\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "# same for validation dataset\n",
    "val_data = val_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=1, remove_columns=['nl', 'code'],\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
    "                                       max_length=512,padding=True, ####new\n",
    "                                       model = model)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28ecb9d10c3aeafd",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"DJANGO-training\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=14,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay=0.01, \n",
    "    warmup_ratio = 0.05,\n",
    "    seed = 1995,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end = True,\n",
    "   \n",
    ")\n",
    "#    \n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=evaluator,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset =val_data,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.01)]\n",
    "\n",
    ")\n",
    "#    save_total_limit=3,\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ae3c58a198400f1",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniele\\PycharmProjects\\NLP-CodeGen\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79996' max='159992' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 79996/159992 2:39:37 < 2:39:37, 8.35 it/s, Epoch 7/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sacrebleu</th>\n",
       "      <th>Bleu-unigram-precision</th>\n",
       "      <th>Bleu-bigram-precision</th>\n",
       "      <th>Bleu-trigram-precision</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Sacre-unigram-precision</th>\n",
       "      <th>Sacre-bigram-precision</th>\n",
       "      <th>Sacre-trigram-precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>0.314930</td>\n",
       "      <td>72.509601</td>\n",
       "      <td>74.410591</td>\n",
       "      <td>87.551677</td>\n",
       "      <td>78.489881</td>\n",
       "      <td>71.778868</td>\n",
       "      <td>72.458506</td>\n",
       "      <td>87.895897</td>\n",
       "      <td>88.106515</td>\n",
       "      <td>80.355252</td>\n",
       "      <td>74.926881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.292700</td>\n",
       "      <td>0.256799</td>\n",
       "      <td>76.525720</td>\n",
       "      <td>78.966626</td>\n",
       "      <td>86.698648</td>\n",
       "      <td>79.282917</td>\n",
       "      <td>73.192436</td>\n",
       "      <td>76.169265</td>\n",
       "      <td>89.944295</td>\n",
       "      <td>87.336463</td>\n",
       "      <td>80.960834</td>\n",
       "      <td>76.210631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.233253</td>\n",
       "      <td>78.632869</td>\n",
       "      <td>80.345448</td>\n",
       "      <td>88.042996</td>\n",
       "      <td>81.243769</td>\n",
       "      <td>76.036343</td>\n",
       "      <td>77.000813</td>\n",
       "      <td>90.530069</td>\n",
       "      <td>88.771251</td>\n",
       "      <td>83.066984</td>\n",
       "      <td>79.089634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.221241</td>\n",
       "      <td>79.816368</td>\n",
       "      <td>81.354769</td>\n",
       "      <td>89.377166</td>\n",
       "      <td>83.154979</td>\n",
       "      <td>78.127244</td>\n",
       "      <td>77.432417</td>\n",
       "      <td>90.829688</td>\n",
       "      <td>89.757990</td>\n",
       "      <td>84.373934</td>\n",
       "      <td>80.481249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.221766</td>\n",
       "      <td>80.694749</td>\n",
       "      <td>82.162807</td>\n",
       "      <td>90.041262</td>\n",
       "      <td>83.845864</td>\n",
       "      <td>78.832117</td>\n",
       "      <td>78.231007</td>\n",
       "      <td>91.208927</td>\n",
       "      <td>90.370899</td>\n",
       "      <td>85.023826</td>\n",
       "      <td>81.146285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.216316</td>\n",
       "      <td>81.272842</td>\n",
       "      <td>82.578538</td>\n",
       "      <td>89.837263</td>\n",
       "      <td>83.916345</td>\n",
       "      <td>78.817524</td>\n",
       "      <td>77.881628</td>\n",
       "      <td>91.154241</td>\n",
       "      <td>90.195483</td>\n",
       "      <td>85.055787</td>\n",
       "      <td>81.097175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.216034</td>\n",
       "      <td>80.832335</td>\n",
       "      <td>82.576287</td>\n",
       "      <td>89.148051</td>\n",
       "      <td>83.103788</td>\n",
       "      <td>78.006152</td>\n",
       "      <td>78.183603</td>\n",
       "      <td>91.301567</td>\n",
       "      <td>89.504256</td>\n",
       "      <td>84.229271</td>\n",
       "      <td>80.199875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "INFO:utils.evaluator:Got BLEU of 72.51 and SacreBLEU of 74.41\n",
      "C:\\Users\\Daniele\\PycharmProjects\\NLP-CodeGen\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:utils.evaluator:Got BLEU of 76.53 and SacreBLEU of 78.97\n",
      "C:\\Users\\Daniele\\PycharmProjects\\NLP-CodeGen\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:utils.evaluator:Got BLEU of 78.63 and SacreBLEU of 80.35\n",
      "C:\\Users\\Daniele\\PycharmProjects\\NLP-CodeGen\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:utils.evaluator:Got BLEU of 79.82 and SacreBLEU of 81.35\n",
      "C:\\Users\\Daniele\\PycharmProjects\\NLP-CodeGen\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:utils.evaluator:Got BLEU of 80.69 and SacreBLEU of 82.16\n",
      "C:\\Users\\Daniele\\PycharmProjects\\NLP-CodeGen\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:utils.evaluator:Got BLEU of 81.27 and SacreBLEU of 82.58\n",
      "C:\\Users\\Daniele\\PycharmProjects\\NLP-CodeGen\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "INFO:absl:Using default tokenizer.\n",
      "INFO:utils.evaluator:Got BLEU of 80.83 and SacreBLEU of 82.58\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=79996, training_loss=0.27258588817216284, metrics={'train_runtime': 9578.5724, 'train_samples_per_second': 16.703, 'train_steps_per_second': 16.703, 'total_flos': 677933400195072.0, 'train_loss': 0.27258588817216284, 'epoch': 7.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#marian-DJANGO-1\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b74b6bc0ac0233e",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
